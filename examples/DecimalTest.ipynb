{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Environment  Loaded\n",
      "20000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-28 15:41:26,250:mylogger:Average Reward of collected trajectories:21.907\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0228 15:41:26.250660 140416558835520 dataset.py:292] Average Reward of collected trajectories:21.907\n",
      "2020-02-28 15:41:26,353:mylogger:Average Reward of collected trajectories:23.585\n",
      "I0228 15:41:26.353203 140416558835520 dataset.py:292] Average Reward of collected trajectories:23.585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward of collected trajectories:21.907\n",
      "Average Reward of collected trajectories:23.585\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import argparse\n",
    "from bigmdp.data.env_gym import SimpleNormalizeEnv\n",
    "from bigmdp.data.dataset import SimpleReplayBuffer, PrioritizedReplayBuffer\n",
    "from bigmdp.utils.utils_log import *\n",
    "from bigmdp.utils.utils_video import *\n",
    "from bigmdp.utils.tmp_vi_helper import *\n",
    "from bigmdp.utils.image_wrappers import *\n",
    "from bigmdp.hyper_params import HYPERPARAMS\n",
    "from bigmdp.utils.utils_directory import *\n",
    "# from async_vi.MDP import *\n",
    "import numpy as np\n",
    "from copy import deepcopy as cpy\n",
    "\n",
    "from bigmdp.mdp.MDP_GPU import FullMDP\n",
    "from model import FC_MDP_Predictor_discrete\n",
    "import pycuda.autoinit\n",
    "\n",
    "# Get all Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bottle_neck_size\", help=\"size\", type=int, default=32)\n",
    "parser.add_argument(\"--discrete_bn\", help=\"size\", type=int, default=0)\n",
    "parser.add_argument(\"--multiplyer\", help=\"multiplyer of feature space\", type=int, default=10)\n",
    "parser.add_argument(\"--env\", help=\"environment name\", type=str, default=\"CartPole\")\n",
    "parser.add_argument(\"--name\", help=\"Experiment name\", type=str, default=\"CartPoleR1\")\n",
    "parser.add_argument(\"--load\", help=\"Load the previous MDP ?\", type=int, default=0)\n",
    "parser.add_argument(\"--symbolic\", help=\"Use Symbolic env if 1 else use image based env\", type=int, default=1)\n",
    "parser.add_argument(\"--steps_to_train\", help=\"Number of steps to train the whole pipeline\", type=int, default=0)\n",
    "parser.add_argument(\"--rmax\", help=\"Use rmax exploration?\", type=int, default=0)\n",
    "parser.add_argument(\"--strict_rmax\", help=\"Use rmax exploration and rmax exploration alone?\", type=int, default=0)\n",
    "parser.add_argument(\"--video_every\", help=\"get a rollout video every\", type=int, default=999999999)\n",
    "parser.add_argument(\"--backup_every\", help=\"do a bellman backup every k frames\", type=int, default=10)\n",
    "parser.add_argument(\"--device\", help=\"do backups on ?\", type=str, default=\"GPU\")\n",
    "parser.add_argument(\"--save_transitions\", help=\"do backups on ?\", type=int, default=0)\n",
    "parser.add_argument(\"--shaped_reward\", help=\"Use shaped rewards?\", type=int, default=0)\n",
    "parser.add_argument(\"--load_time_string\", help=\"timestep string\", type=str, default=\"default_time\")\n",
    "parser.add_argument(\"--use_priority_buffer\", help=\"Use priority buffer ?\", type=int, default=0)\n",
    "parser.add_argument(\"--render_every\", help=\"render a episode every kth episode\", type=int, default=0)\n",
    "parser.add_argument(\"--internal_count_override\", help=\"override internal count\", type=int, default=0)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(\"--video_every 50 --env CartPole --multiplyer 5 --rmax 0 --backup_every 100 --load_time_string decimal_test\".split(\" \"))\n",
    "# args = parser.parse_args(\"--video_every 50 --env Acrobot --multiplyer 5 --rmax 0 --backup_every 5 --load_time_string without_prior\".split(\" \"))\n",
    "\n",
    "\n",
    "run_params = \"_bn-\" + str(args.bottle_neck_size) + \\\n",
    "             \"_sym-\" + str(args.symbolic) + \\\n",
    "             \"_rmax-\" + str(bool(args.rmax)) + \\\n",
    "             \"_strict_rmax-\" + str(bool(args.strict_rmax)) + \\\n",
    "             \"_mult-\" + str(args.multiplyer) + \\\n",
    "             \"_bkp_f-\" + str(args.backup_every) + \\\n",
    "             \"_device-\" + str(args.device) + \\\n",
    "             \"_priority-\" + str(args.use_priority_buffer)\n",
    "\n",
    "base_file_path = \"./result_dump/{}/{}\".format(args.env, args.name + run_params)\n",
    "\n",
    "create_hierarchy(base_file_path)\n",
    "\n",
    "train_epochs = 20\n",
    "\n",
    "log_dirs_dict, loggers_dict = get_advanced_log_dir_and_logger(ROOT_FOLDER=\"Symbolic\" if args.symbolic else \"Image\",\n",
    "                                                              EXP_ID=args.env + \"-Dec\", #todo remove for general\n",
    "                                                              EXP_PARAMS=run_params,\n",
    "                                                              load_time_string=args.load_time_string,\n",
    "                                                              tb_log_keys=[\"tb_train_logger\", \"tb_valid_logger\"])\n",
    "\n",
    "if args.symbolic:\n",
    "    params = HYPERPARAMS[args.env + \"-sym\"]\n",
    "    if os.path.exists(\"./\" + str(args.env) + \"_env.pk\"):\n",
    "        print(\" Environment  Loaded\")\n",
    "        env = torch.load(\"./\" + str(args.env) + \"_env.pk\")\n",
    "    else:\n",
    "        env = SimpleNormalizeEnv(params[\"env_name\"], max_episode_length=params[\"max_episode_length\"])\n",
    "        torch.save(env, \"./\" + str(args.env) + \"_env.pk\")\n",
    "else:\n",
    "    print(\"Not Implemented Yet\")  # Todo\n",
    "    params = HYPERPARAMS[args.env + \"-img\"]\n",
    "    assert False\n",
    "\n",
    "test_env = cpy(env)\n",
    "\n",
    "\n",
    "K_s, K_i = params[\"replay_initial\"], args.internal_count_override if args.internal_count_override else params[\n",
    "    'internal_step_count_per_policy']\n",
    "print(K_i, K_s)\n",
    "\n",
    "if args.load:\n",
    "    mdp = torch.load(base_file_path + \"mdp_class.pth\")\n",
    "else:\n",
    "    mdp = FullMDP(A=env.get_list_of_actions(),\n",
    "                  ur=params[\"unknown_transition_reward\"],\n",
    "                  vi_params={\"gamma\": params[\"gamma\"],\n",
    "                             \"slip_prob\": params[\"slip_probability\"],\n",
    "                             \"rmax_reward\": params[\"rmax_reward\"],\n",
    "                             \"rmax_thres\": 2,\n",
    "                             \"balanced_explr\": True,\n",
    "                             \"rmin\": params[\"rmin\"]},\n",
    "                  policy_params={\"unhash_array_len\": env._env.observation_space.shape[0]},\n",
    "                  MAX_S_COUNT=int(1e6),\n",
    "                  weight_transitions=False,\n",
    "                  default_mode = args.device)\n",
    "\n",
    "all_rewards = []\n",
    "eval_rewards, safe_eval_rewards = [], []\n",
    "policy_fetch_time = []\n",
    "bellman_backup_time = [9999]\n",
    "train_buffer = PrioritizedReplayBuffer(int(5e5)) if args.use_priority_buffer else SimpleReplayBuffer(int(5e5))\n",
    "valid_buffer = PrioritizedReplayBuffer(int(1e5)) if args.use_priority_buffer else SimpleReplayBuffer(int(5e5))\n",
    "\n",
    "eps_tracker = EpsilonTracker(params)\n",
    "\n",
    "frame_count = 0\n",
    "warmup_eps = 10\n",
    "eval_reward = 0\n",
    "\n",
    "omit_list = [\"end_state\", \"unknown_state\"]\n",
    "\n",
    "from bigmdp.data.dataset import gather_data_in_buffer\n",
    "from learnt_mdp_helper import *\n",
    "\n",
    "# Collect random Dataset # till replay initial\\\n",
    "random_policy = lambda s: env.sample_random_action()\n",
    "train_buffer, info = gather_data_in_buffer(train_buffer, env, episodes=9999, render=False, policy=random_policy,\n",
    "                                           frame_count=K_s, pad_attribute_fxn={\"qval\": lambda s: 0})\n",
    "valid_buffer, info = gather_data_in_buffer(valid_buffer, env, episodes=9999, render=False, policy=random_policy,\n",
    "                                           frame_count=int(K_s / 5), pad_attribute_fxn={\"qval\": lambda s: 0})\n",
    "\n",
    "# recon_loss_wts = {h:0 for h in training_net.head_ids}\n",
    "# recon_loss_wts[\"recon\"] = 1\n",
    "# training_net.default_loss_wts  = recon_loss_wts\n",
    "# training_net.fit(train_buffer,valid_buffer, epochs=50)\n",
    "frame_count = 0\n",
    "print(frame_count)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.model_update_every = 500\n",
    "\n",
    "outer_loop_frame_count = len(train_buffer)\n",
    "\n",
    "    # torch.save((train_buffer.buffer, mdp, net.state_dict()), log_dirs_dict[\"py_log_dir\"] + \"/checkpoint\" + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"evaluate_every\"] = 5000\n",
    "params[\"outer_iteration_count\"] = 1\n",
    "params[\"bellman_backup_every\"] = 100\n",
    "params[\"n_backups\"] = 10\n",
    "K_i = int(2e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_2_disc_fxn(s):\n",
    "    if len(s)==1:\n",
    "        return s[0] * args.multiplyer\n",
    "    else:\n",
    "        return [s_ * args.multiplyer for s_ in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:05<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of MDP, 496\n",
      "Time takedn to solve 2.43491792678833\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outer_count = 0 \n",
    "# Update the buffer with new values\n",
    "# For Disc: Skipped\n",
    "\n",
    "# Train your Network\n",
    "# For Disc: Skipped\n",
    "\n",
    "# Populate your MDP\n",
    "mdp_T = make_new_mdp(train_buffer, img_2_disc_fxn, params, env)\n",
    "print(\"Size of MDP,\", len(mdp_T.tD))\n",
    "\n",
    "# Solve Your MDP\n",
    "mdp_T.solve(eps=1, mode=args.device)\n",
    "\n",
    "# for mdp_name, M in {\"Tabular\": mdp_T, \"Tabular_Insertion\": mdp_TI}.items():\n",
    "\n",
    "opt_policy = lambda s: mdp_T.get_opt_action(hAsh(img_2_disc_fxn([s]).tolist()), mode = args.device)\n",
    "safe_policy = lambda s: mdp_T.get_safe_action(hAsh(img_2_disc_fxn([s]).tolist()), mode = args.device)\n",
    "explr_policy = lambda s: mdp_T.get_explr_action(hAsh(img_2_disc_fxn([s]).tolist()), mode = args.device)\n",
    "random_policy = lambda s: env.sample_random_action()\n",
    "eps_opt_policy = get_eps_policy(opt_policy, random_policy, epsilon=0.2)\n",
    "\n",
    "if args.strict_rmax:\n",
    "    explore_policies = {\"explr_policy\": explr_policy}\n",
    "elif args.rmax:\n",
    "    explore_policies = {\"explr_policy\": explr_policy, \"opt_policy\": opt_policy}\n",
    "else:\n",
    "    explore_policies = {\"opt_policy\": opt_policy, \"eps_opt_policy\":eps_opt_policy}\n",
    "\n",
    "s = env.reset()\n",
    "running_reward = 0\n",
    "inner_loop_frame_count = 0\n",
    "eps_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.render_every = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5007/100000000 [00:27<3085:17:30,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "78.84          | 657         | 343/1314              | 0.261      | 0.39984130859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10007/100000000 [00:55<2869:20:35,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "116.4          | 854         | 495/1708              | 0.2898     | 0.593780517578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15006/100000000 [01:22<3054:40:43,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "176.52         | 974         | 571/1948              | 0.2931     | 0.554718017578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20005/100000000 [01:51<3336:50:19,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "122.5          | 1092        | 645/2184              | 0.2953     | 1.31256103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25000/100000000 [02:20<4457:39:47,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "131.58         | 1162        | 681/2324              | 0.293      | 1.6876220703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30000/100000000 [02:50<4529:51:08,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "88.3           | 1277        | 755/2554              | 0.2956     | 32.89703369140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 35009/100000000 [03:17<2716:55:11, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "163.58         | 1325        | 765/2650              | 0.2887     | 12.09625244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40008/100000000 [03:48<3693:59:35,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "250.24         | 1377        | 798/2754              | 0.2898     | 4.44677734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45008/100000000 [04:17<3323:52:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "283.62         | 1450        | 847/2900              | 0.2921     | 1.63446044921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50006/100000000 [04:45<2958:56:29,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "148.42         | 1513        | 886/3026              | 0.2928     | 11.619873046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 55007/100000000 [05:14<3106:44:40,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "232.22         | 1574        | 918/3148              | 0.2916     | 3.362060546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60007/100000000 [05:44<3599:48:12,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "236.82         | 1642        | 959/3284              | 0.292      | 1.23675537109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 65006/100000000 [06:12<3115:49:35,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "223.2          | 1738        | 1024/3476             | 0.2946     | 0.57220458984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 70005/100000000 [06:41<3152:14:46,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "287.1          | 1825        | 1082/3650             | 0.2964     | 1.3369140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 75000/100000000 [07:10<4473:34:34,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward | MDP #States | #MissingTrans/ #Trans | Missing %. | VI Error..\n",
      "181.58         | 1853        | 1091/3706             | 0.2944     | 0.7327880859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 78177/100000000 [07:22<96:14:21, 288.41it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(int(1e8))):\n",
    "    inner_loop_frame_count += 1\n",
    "    outer_loop_frame_count += 1\n",
    "\n",
    "    if args.render_every and eps_count % args.render_every == 0:\n",
    "        env.render()\n",
    "\n",
    "    if outer_loop_frame_count % params['bellman_backup_every'] == 0:\n",
    "        st = time.time()\n",
    "        mdp_T.do_optimal_backup(mode=args.device, n_backups=params[\"n_backups\"])\n",
    "        if args.rmax:\n",
    "            mdp_T.do_explr_backup(mode=args.device, n_backups=params[\"n_backups\"])\n",
    "        bellman_backup_time.append(time.time() - st)\n",
    "\n",
    "    policy_name, policy = \"a\", eps_opt_policy # list(explore_policies.items())[eps_count%len(explore_policies)]\n",
    "    a = policy(s)\n",
    "\n",
    "    ns, r, d, i = env.step(a)\n",
    "    _d = False if d and i[\"max_episode_length_exceeded\"] == True else d\n",
    "    running_reward += r\n",
    "\n",
    "    # add to buffer\n",
    "    # Not necessary for Decimal Discretization bur sure\n",
    "    exp = [s.tolist(), [a], ns.tolist(), [r], [_d]]\n",
    "    train_buffer.add(exp, padded_info={\"qval\": 0})\n",
    "    if np.random.randint(20) < 2:\n",
    "        valid_buffer.add(exp, padded_info={\"qval\": 0})\n",
    "\n",
    "    # Update MDP\n",
    "    s_d, ns_d = img_2_disc_fxn([s]), img_2_disc_fxn([ns])\n",
    "    hs_d, hns_d = hAsh(s_d.tolist()), hAsh(ns_d.tolist())\n",
    "    mdp_T.consume_transition([hs_d, int(a), hns_d, float(r), int(_d)])\n",
    "#     mdp_T.consume_transition((hAsh(hs.tolist()), int(a), hAsh(hns.tolist()), float(r), int(_d)))\n",
    "\n",
    "    # prep for next loop\n",
    "    s = ns\n",
    "\n",
    "    # Housekeeping to omit while true loop\n",
    "    if d:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        eps_count += 1\n",
    "\n",
    "    if inner_loop_frame_count % params['evaluate_every'] == 0:\n",
    "        mdp_T.do_optimal_backup(mode=\"GPU\", n_backups=500)\n",
    "        n_steps = outer_count * K_i + inner_loop_frame_count\n",
    "        eval_reward = evaluate_on_env(test_env, opt_policy, eps_count=50, render=False)[0]\n",
    "        \n",
    "        col_header = [\"Average Reward\",\"MDP #States\", \"#MissingTrans/ #Trans\",\"Missing %\", \"VI Error\"] \n",
    "        meta_data = [10]*len(col_header) # max len of the data in the table to be printed (per column)\n",
    "        print(' | '.join([s.ljust(max(meta_data[i], len(s)), '.') for i, s in enumerate(col_header)]))\n",
    "        col_data = [eval_reward, \n",
    "                    len(mdp_T.tD),  \n",
    "                    str(mdp_T.missing_state_action_count)+\"/\"+str(len(mdp_T.tD)*len(mdp_T.A)),\n",
    "                    round(mdp_T.missing_state_action_count/mdp_T.total_state_action_count,4 ),\n",
    "                    mdp_T.curr_vi_error       ]\n",
    "        print(' | '.join([str(s).ljust(max(meta_data[i], len(col_header[i])),  \" \") for i, s in enumerate(col_data)]))\n",
    "        \n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar(\"Optimal Avg Reward\", n_steps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar(\"State Count\", float(len(mdp_T.tD)), outer_loop_frame_count)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Mising Transition Count', mdp.missing_state_action_count, outer_loop_frame_count)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar(\"Opt VI Error\",mdp_T.curr_vi_error, outer_loop_frame_count)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar(\"Safe VI Error\",mdp_T.s_curr_vi_error, outer_loop_frame_count)\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    # Not necessary for Decimal Experiments\n",
    "    # if outer_loop_frame_count % args.checkpoint_every == 0:\n",
    "    #     training_net._save_to_cache()\n",
    "    #     torch.save((train_buffer.buffer, mdp, net.state_dict()),\n",
    "    #                log_dirs_dict[\"py_log_dir\"] + \"/checkpoint\" + \".pth\")\n",
    "\n",
    "# Not necessary for Decimal Experiments\n",
    "# training_net._save_to_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
