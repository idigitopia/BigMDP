{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import argparse\n",
    "from bigmdp.data.env_gym import SimpleNormalizeEnv\n",
    "from bigmdp.data.dataset import SimpleReplayBuffer\n",
    "from bigmdp.utils.utils_log import *\n",
    "from bigmdp.utils.utils_video import *\n",
    "from bigmdp.utils.tmp_vi_helper import *\n",
    "from bigmdp.utils.image_wrappers import *\n",
    "from bigmdp.hyper_params import HYPERPARAMS\n",
    "from bigmdp.utils.utils_directory import *\n",
    "# from async_vi.MDP import *\n",
    "import numpy as np\n",
    "\n",
    "from bigmdp.mdp.MDP_GPU import FullMDP\n",
    "import pycuda.autoinit\n",
    "\n",
    "# Get all Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bottle_neck_size\", help=\"size\", type=int, default=32)\n",
    "parser.add_argument(\"--discrete_bn\", help=\"size\", type=int, default=0)\n",
    "parser.add_argument(\"--multiplyer\", help=\"multiplyer of feature space\", type=int, default=10)\n",
    "parser.add_argument(\"--env\", help=\"environment name\", type=str, default=\"CartPole\")\n",
    "parser.add_argument(\"--name\", help=\"Experiment name\", type=str, default=\"CartPoleR1\")\n",
    "parser.add_argument(\"--load\", help=\"Load the previous MDP ?\", type=int, default=0)\n",
    "parser.add_argument(\"--symbolic\", help=\"Use Symbolic env if 1 else use image based env\", type=int, default=1)\n",
    "parser.add_argument(\"--steps_to_train\", help=\"Number of steps to train the whole pipeline\", type=int, default=0)\n",
    "parser.add_argument(\"--rmax\", help=\"Use rmax exploration?\", type=int, default=0)\n",
    "parser.add_argument(\"--strict_rmax\", help=\"Use rmax exploration and rmax exploration alone?\", type=int, default=0)\n",
    "parser.add_argument(\"--video_every\", help=\"get a rollout video every\", type=int, default= 999999999)\n",
    "parser.add_argument(\"--backup_every\", help=\"do a bellman backup every k frames\", type=int, default= 10)\n",
    "parser.add_argument(\"--device\", help=\"do backups on ?\", type=str, default= \"GPU\")\n",
    "parser.add_argument(\"--save_transitions\", help=\"do backups on ?\", type=int, default= 0)\n",
    "\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(\"--video_every 50 --env CartPole --multiplyer 100 --rmax 1 --backup_every 5\".split(\" \"))\n",
    "run_params = \"_bn-\" + str(args.bottle_neck_size) +\\\n",
    "            \"_sym-\" + str(args.symbolic) + \\\n",
    "            \"_rmax-\" + str(bool(args.rmax)) +\\\n",
    "            \"_strict_rmax-\" + str(bool(args.strict_rmax)) +\\\n",
    "            \"_mult-\" + str(args.multiplyer) +\\\n",
    "            \"_bkp_f-\" + str(args.backup_every) +\\\n",
    "            \"_device-\" + str(args.device)\n",
    "                            \n",
    "base_file_path = \"./result_dump/{}/{}\".format(args.env, args.name + run_params)\n",
    "\n",
    "create_hierarchy(base_file_path)\n",
    "\n",
    "def latent_to_hs_disc_fxn(s):\n",
    "    if len(s)==1:\n",
    "        return hAsh(s[0] * args.multiplyer)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "log_dirs_dict, loggers_dict = get_advanced_log_dir_and_logger(ROOT_FOLDER = \"Symbolic\" if args.symbolic else \"Image\",\n",
    "                                                             EXP_ID = args.env,\n",
    "                                                             EXP_PARAMS = run_params,\n",
    "                                                             tb_log_keys=[\"tb_train_logger\", \"tb_vi_logger\"])\n",
    "\n",
    "\n",
    "if args.symbolic:\n",
    "    params = HYPERPARAMS[args.env + \"-sym\"]\n",
    "\n",
    "    if args.env == \"CartPole\" and os.path.exists(\"./cartpole_env.pk\"):\n",
    "        print(\" Environment  Loaded\")\n",
    "        env = torch.load(\"cartpole_env.pk\")\n",
    "    elif args.env == \"CartPole\":\n",
    "        env = SimpleNormalizeEnv(params[\"env_name\"], max_episode_length=params[\"max_episode_length\"])\n",
    "        torch.save(env,\"cartpole_env.pk\")\n",
    "    else:\n",
    "        env = SimpleNormalizeEnv(params[\"env_name\"], max_episode_length=params[\"max_episode_length\"])\n",
    "else:\n",
    "    print(\"Not Implemented Yet\")  # Todo\n",
    "    params = HYPERPARAMS[args.env + \"-img\"]\n",
    "    assert False\n",
    "\n",
    "if args.load:\n",
    "    mdp = torch.load(base_file_path + \"mdp_class.pth\")\n",
    "else:\n",
    "    mdp = FullMDP(A=env.get_list_of_actions(),\n",
    "                  ur=params[\"unknown_transition_reward\"],\n",
    "                  vi_params={\"gamma\": params[\"gamma\"],\n",
    "                             \"slip_prob\": params[\"slip_probability\"],\n",
    "                             \"rmax_reward\": params[\"rmax_reward\"],\n",
    "                             \"rmax_thres\": 2,\n",
    "                             \"balanced_explr\": True,\n",
    "                             \"rmin\": params[\"rmin\"]},\n",
    "                  policy_params={\"unhash_array_len\":env._env.observation_space.shape[0]},\n",
    "                  MAX_S_COUNT=int(2e5))\n",
    "\n",
    "all_rewards = []\n",
    "eval_rewards, safe_eval_rewards = [], []\n",
    "policy_fetch_time = []\n",
    "bellman_backup_time = [9999]\n",
    "tran_buffer = SimpleReplayBuffer(int(params[\"replay_size\"]))\n",
    "eps_tracker = EpsilonTracker(params)\n",
    "\n",
    "frame_count = 0\n",
    "warmup_eps = 10\n",
    "eval_reward = 0\n",
    "\n",
    "safe_policy = lambda s : mdp.get_safe_action(latent_to_hs_disc_fxn([s]))\n",
    "opt_policy =  lambda s : mdp.get_opt_action(latent_to_hs_disc_fxn([s]))\n",
    "explr_policy =  lambda s : mdp.get_explr_action(latent_to_hs_disc_fxn([s]))\n",
    "random_policy = lambda s: env.sample_random_action()\n",
    "\n",
    "\n",
    "params[\"replay_initial\"] = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18215"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n",
      "-====-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for eps in range(0, 100000):\n",
    "    s = env.reset()\n",
    "    running_reward = 0\n",
    "\n",
    "    while True:\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count > params[\"replay_initial\"] and frame_count % args.backup_every == 0:\n",
    "            st = time.time()\n",
    "            mdp.do_optimal_backup(mode=args.device, n_backups=10)\n",
    "            mdp.do_explr_backup(mode=args.device, n_backups=10)\n",
    "            mdp.do_safe_backup(mode=args.device, n_backups=10)\n",
    "            bellman_backup_time.append(time.time() - st)\n",
    "\n",
    "        st = time.time()\n",
    "        if frame_count < (params[\"replay_initial\"] + 100) or (\n",
    "                np.random.random() < eps_tracker.get_eps(frame_count) and not args.rmax):\n",
    "            a = random_policy(s)\n",
    "        else:\n",
    "            a = explr_policy(s) if args.strict_rmax or (args.rmax and eps % 2 == 0) else opt_policy(s)\n",
    "        policy_fetch_time.append(time.time() - st)\n",
    "\n",
    "        ns, r, d, i = env.step(a)\n",
    "        _d = False if d and i[\"max_episode_length_exceeded\"] == True else d\n",
    "\n",
    "        hs_d, hns_d = latent_to_hs_disc_fxn([s]), latent_to_hs_disc_fxn([ns])\n",
    "        #         shared_store_1.add_to_transition_queue.remote([hs_d, a, hns_d, r, _d])\n",
    "        mdp.consume_transition([hs_d, a, hns_d, r, _d])\n",
    "        tran_buffer.add([s, a, ns, r, _d])\n",
    "\n",
    "        running_reward += r\n",
    "        s = ns\n",
    "\n",
    "#         if frame_count > params[\"replay_initial\"] and eps % 10 == 0:\n",
    "#             env.render()\n",
    "\n",
    "        if d:\n",
    "            if eps % 50 == 0:\n",
    "                print(\"-====-\")\n",
    "            break\n",
    "\n",
    "\n",
    "        if frame_count % params['checkpoint_every'] == 0 :\n",
    "            torch.save(tran_buffer, log_dirs_dict[\"py_log_dir\"] + \"/all_transitions.pth\")\n",
    "            torch.save(mdp, log_dirs_dict[\"py_log_dir\"] + \"/mdp.pth\")\n",
    "\n",
    "\n",
    "    all_rewards.append(running_reward)\n",
    "\n",
    "    #### Evaluation Code\n",
    "    if frame_count > (params[\"replay_initial\"] + 2 * args.backup_every) and frame_count > params['checkpoint_every']:\n",
    "\n",
    "        #Calculate parameters\n",
    "        rmax_count = sum([1 for s in mdp.tC for a in mdp.tC[s] if sum(mdp.tC[s][a].values()) < 10])\n",
    "        #         vi_error, e_vi_error, s_vi_error = ray.get(shared_store_1.get_curr_vi_errors.remote())\n",
    "        vi_error, e_vi_error, s_vi_error = [mdp.curr_vi_error, mdp.e_curr_vi_error, mdp.s_curr_vi_error]\n",
    "\n",
    "        eval_reward = evaluate_on_env(env, opt_policy, eps_count=2, render=False)[0]\n",
    "        eval_rewards.append(eval_reward)\n",
    "\n",
    "        safe_eval_reward = evaluate_on_env(env, safe_policy, eps_count=2, render=False)[0]\n",
    "        safe_eval_rewards.append(safe_eval_reward)\n",
    "\n",
    "\n",
    "        # Log stuffs in Tensorboard\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Safe policy performance', float(safe_eval_reward), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Optimal policy performance', float(eval_reward), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Expl/Expt policy performance', float(running_reward), eps)\n",
    "\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('MDP State Count', float(len(mdp.vD)), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Rmax Count', float(rmax_count), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_histogram('Optimal Value Distr', torch.tensor(list(mdp.vD.values())),\n",
    "                                                      eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_histogram('Optimal Policy Distr', torch.tensor(list(mdp.pD.values())),\n",
    "                                                      eps)\n",
    "\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Expl VI_error', float(e_vi_error), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Opt VI_error', float(vi_error), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Safe VI_error', float(s_vi_error), eps)\n",
    "\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Explr Epsilon', float(eps_tracker.get_eps(frame_count)), eps)\n",
    "        loggers_dict[\"tb_train_logger\"].add_scalar('Policy Fetch rate', float(mean(policy_fetch_time)), eps)\n",
    "\n",
    "\n",
    "        # Print on the Console\n",
    "        print(\"episode:\", eps,\n",
    "              \"reward:\", running_reward,\n",
    "              #               \"Bkp error:\", [round(d,6) for d in ray.get(shared_store_1.get_curr_vi_errors.remote())],\n",
    "              \"Bkp error:\", [round(d, 6) for d in [mdp.curr_vi_error, mdp.e_curr_vi_error, mdp.s_curr_vi_error]],\n",
    "              \"s8 visited:\", len(mdp.tC),\n",
    "              \"Rmax S8 Count\", rmax_count,\n",
    "              \"epsilon\", round(eps_tracker.get_eps(frame_count), 2),\n",
    "              \"Eval Reward\", eval_reward, safe_eval_reward,\n",
    "              \"policy_fetch_rate\", round(mean(policy_fetch_time), 6),\n",
    "              \"bellman_backup_time\", round(mean(bellman_backup_time), 6) if bellman_backup_time else 0\n",
    "              )\n",
    "\n",
    "        policy_fetch_time = []\n",
    "        bellman_backup_time = []\n",
    "\n",
    "        # Make Video\n",
    "        if eps % args.video_every == 0 and eps != 0:\n",
    "            nn_performance, info_, video = rollout_with_nn_behavior(env=env,\n",
    "                                                                    policy=explr_policy if args.rmax else opt_policy,\n",
    "                                                                    hs_nn_fxn=lambda hs: mdp._get_nn_hs(hs),\n",
    "                                                                    pi_dict=mdp.pD,\n",
    "                                                                    v_dict=mdp.e_vD if args.rmax else mdp.vD,\n",
    "                                                                    hs_st_disc_fxn=latent_to_hs_disc_fxn,\n",
    "                                                                    A=env.get_list_of_actions(),\n",
    "                                                                    tranDict=mdp.tD,\n",
    "                                                                    rewardDict=mdp.e_rD if args.rmax else mdp.rD,\n",
    "                                                                    eps=2, render=False)\n",
    "            save_video(video, title=\"rollout_t_DECODE_NN_\" + str(eps), base_path=log_dirs_dict[\"py_log_dir\"])\n",
    "\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
